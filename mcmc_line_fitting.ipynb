{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Line Using MCMC\n",
    "By: Griffin Hosseinzadeh (2019 April 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Some Fake Data\n",
    "Choose a slope $m$, intercept $b$, and scatter $\\sigma$, and generate $N$ random points using the `np.random` module. Plot the results to see what your data look like. Our goal is to recover the parameters defined here. (Hint: this section is identical to the previous notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Down the Posterior\n",
    "Define functions that return the prior, the likelihood, and the posterior given a parameter vector `theta = [m, b]`. For computational accuracy (and convenience), we actually want to do this in log space. You can either use `np.log` (natural log) or `np.log10` (base-10 log). Regardless of the shape of your prior, it's good practice to limit the values of $m$ and $b$ to some reasonable range. (Hint: this section is identical to the previous notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "def log_prior(# complete\n",
    "\n",
    "def log_likelihood(# complete\n",
    "\n",
    "def log_posterior(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Ensemble Sampler\n",
    "This time we will use the `emcee` package that you imported above to sample the posterior probability distribution. Everything you need is contained within the `emcee.EnsembleSampler` class. Initialize this with 100 walkers and the `log_posterior` function you defined above. Don't forget that `log_posterior` takes additional arguments `x`, `y`, and `dy`. (Hint: check the documentation for `EnsembleSampler` for how to deal with this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walkers = 100\n",
    "n_params = # complete\n",
    "args = # complete\n",
    "\n",
    "sampler = emcee.EnsembleSampler(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!\n",
    "Now that the `EnsembleSampler` object is initialized, all you need to do is run it with the `run_mcmc` method. You'll need to give initial guesses for each walker, which you can draw randomly from the prior. For now, 10000 steps is more than enough, and should run in less than a minute on your laptop. If this command takes too long, ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_random = # complete\n",
    "b_random = # complete\n",
    "initial_guesses = # complete\n",
    "\n",
    "sampler.run_mcmc(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results\n",
    "First make a plot of the walker positions (value of each parameter) at each step. Write a function to do this, since you'll be using it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.chain.shape)  # check that this has dimensions (n_walkers, n_steps, n_params)\n",
    "\n",
    "def chain_plots(chain, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot the histories for each walker in sampler.chain\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    chain: ndarray\n",
    "        3D array given by sampler.flatchain, with shape (n_walkers, n_steps, n_params)\n",
    "    kwargs: `.Line2D` properties, optional\n",
    "        All keyword arguments are passed to `pyplot.plot`\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(# complete\n",
    "    plt.xlabel('Step Number')\n",
    "    plt.ylabel('Slope')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(# complete\n",
    "    plt.xlabel('Step Number')\n",
    "    plt.ylabel('Intercept')\n",
    "    \n",
    "chain_plots(sampler.chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of those colored lines is a walker. They started out at your initial guesses. What do you immediately notice?\n",
    "\n",
    "Try making more useful plots by zooming in on the relevant parts. Hint: you can adjust line opacity in matplotlib with the `alpha` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burn = # complete\n",
    "chain_plots(# complete\n",
    "chain_plots(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your chains converged? How many steps did it take for this to happen? This is called the \"burn-in\" period. **From now on, only use the steps after the burn-in period.**\n",
    "\n",
    "Now make a scatter plot of the two parameters, one point for each walker-step. Hint: `sampler.flatchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the distribution you had in the previous notebook (assuming you used the same parameters)?\n",
    "\n",
    "Now let's marginalize over each parameter. Given that we have walkers that are distributed according to the 2D posterior, think about how to calculate the marginalized posterior for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# complete\n",
    "\n",
    "plt.figure()\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an easy way to make all these plots: `corner.corner`! Produce a corner plot with the true parameters labeled and the best-fit values (with error bars) printed on the plot. How close did we get to the parameters we used to generate the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-Away Message\n",
    "MCMC is a much more efficient way to sample the probability distribution because the walkers mostly stay in the highest-probability region. Plus it essentially gives you the marginalized posteriors for free, because the density of walkers matches the probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Things to Try\n",
    "- Add a parameter for the intrinsic scatter and redo the MCMC fit.\n",
    "- Try implementing Metropolis-Hastings algorithm yourself and see if you can get the same results as `emcee`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
