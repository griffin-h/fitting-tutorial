{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Line to Some Noisy Data\n",
    "By: Griffin Hosseinzadeh (2019 April 17), with additions by Charlotte Mason (2020 August 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Some Fake Data\n",
    "Choose a slope $m$, intercept $b$, and scatter $\\sigma$, and generate $N$ random points using the `np.random` module. Plot the results to see what your data look like. Our goal is to recover the parameters defined here. (You are free to change them to whatever you want, but be aware that your plots may look different than others'.)\n",
    "\n",
    "Straight line model:\n",
    "\n",
    "$$ y = mx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
   },
   "outputs": [],
   "source": [
    "N = 100  # number of points\n",
    "m = 10.  # slope\n",
    "b = 50.  # intercept\n",
    "sigma = 1.  # scatter\n",
    "\n",
    "x = np.random.rand(N)  # random values between 0 and 1\n",
    "y = m * x + b + sigma * np.random.randn(N)  # random values from a Gaussian centered at 0 with standard deviation 1\n",
    "dy = np.repeat(sigma, N) # errorbars for all the data\n",
    "plt.errorbar(x, y, dy, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Down the Posterior\n",
    "Define functions that return the prior, the likelihood, and the posterior given a parameter vector `theta = [m, b]`. For computational accuracy (and convenience), we actually want to do this in log space. You can either use `np.log` (natural log) or `np.log10` (base-10 log). Regardless of the shape of your prior, it's good practice to limit the values of $m$ and $b$ to some reasonable range.\n",
    "\n",
    "**Bayes Theorem**\n",
    "\n",
    "The posterior is proportional to the likelihood $\\mathcal{L}$ times the prior on the parameters $\\theta = m, b$:\n",
    "\n",
    "$$ p(\\theta | \\mathrm{data}) \\propto \\mathcal{L}(\\mathrm{data} | \\theta) \\times p(\\theta) $$\n",
    "\n",
    "**Likelihood**\n",
    "\n",
    "Assuming Gaussian errors, the likelihood for each data point given the model with parameters $\\theta$ will be:\n",
    "\n",
    "$$ \\mathcal{L}_i(y_i | \\theta, x_i, y_{\\mathrm{err},i}) = \\frac{1}{\\sqrt{2\\pi y_{\\mathrm{err},i}^2}} \\exp\\left[-\\frac{(y_i - y_\\mathrm{model}(x_i, \\theta))^2}{2 y_{\\mathrm{err},i}^2}\\right] $$\n",
    "\n",
    "Or in log space:\n",
    "\n",
    "$$ \\ln \\mathcal{L}_i(y_i | \\theta, x_i, y_{\\mathrm{err},i}) = -\\frac{1}{2} \\left( \\ln{2\\pi y_{\\mathrm{err},i}^2}  + \\left[\\frac{y_i - y_\\mathrm{model}(x_i, \\theta)}{y_{\\mathrm{err},i}^2}\\right]^2 \\right) $$\n",
    "\n",
    "Assuming the data are independent, the likelihood for obtaining all of the data points is the product of the individual likelihoods:\n",
    "\n",
    "$$ \\mathcal{L}(\\{y_i\\} | \\theta, \\{x_i, y_{\\mathrm{err},i}\\}) = \\prod_i \\mathcal{L}_i(y_i | \\theta, x_i, y_{\\mathrm{err},i}) $$\n",
    "\n",
    "Or in log space you can sum the log likelihoods:\n",
    "\n",
    "$$ \\ln \\mathcal{L} = \\ln \\left(\\prod_i \\mathcal{L}_i \\right) = \\sum_i \\ln \\mathcal{L}_i $$\n",
    "\n",
    "**Prior**\n",
    "\n",
    "The simplest prior is a uniform prior to restrict the parameters to some range, e.g.:\n",
    "\n",
    "$$ m = \\begin{cases}\n",
    "1, & 0 < m < 100\\\\\n",
    "0, & \\mathrm{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_min = 0.\n",
    "m_max = 100.\n",
    "b_min = 0.\n",
    "b_max = 100.\n",
    "\n",
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    Returns log(prior) for a given parameter vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: list, array-like\n",
    "        List of parameters in the form [slope, intercept]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ln_prior: float\n",
    "        Natural log of the prior probability function\n",
    "    \"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return ln_prior\n",
    "\n",
    "\n",
    "def log_likelihood(theta, x, y, dy):\n",
    "    \"\"\"complete\"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return # complete\n",
    "\n",
    "\n",
    "def log_posterior(theta, x, y, dy):\n",
    "    \"\"\"complete\"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_min = 0.\n",
    "m_max = 100.\n",
    "b_min = 0.\n",
    "b_max = 100.\n",
    "\n",
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    Returns log(prior) for a given parameter vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: list, array-like\n",
    "        List of parameters in the form [slope, intercept]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ln_prior: float\n",
    "        Natural log of the prior probability function\n",
    "    \"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return ln_prior\n",
    "\n",
    "\n",
    "def log_likelihood(theta, x, y, dy):\n",
    "    \"\"\"complete\"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return # complete\n",
    "\n",
    "\n",
    "def log_posterior(theta, x, y, dy):\n",
    "    \"\"\"complete\"\"\"\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it on a Grid\n",
    "Now that you have defined `log_posterior`, produce a grid of $m$ and $b$ values, and evaluate the posterior at each point on the grid. Plot the results using `plt.contour` (contour plot) or `plt.contourf` (filled contour plot). Plot the input values of $m$ and $b$ as a point to see where they sit in the posterior space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_spacing = 1.\n",
    "m_range = np.arange(m_min, m_max, grid_spacing)\n",
    "b_range = np.arange(b_min, b_max, grid_spacing)\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "posterior_grid = # complete\n",
    "\n",
    "plt.contourf( # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_spacing = 1.\n",
    "m_range = # complete\n",
    "b_range = # copmlete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "posterior_grid = # complete\n",
    "\n",
    "plt.contourf( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the parameters that maximize the posterior distribution. How close did we come to the parameters we used to generate the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices where the posterior is maximised --> what m and b is this?\n",
    "i_max = # complete\n",
    "j_max = # complete\n",
    "\n",
    "m_infer = m_range[i_max]\n",
    "b_infer = b_range[j_max]\n",
    "print(f'm = {m_infer:.1f}, b = {b_infer:.1f}')\n",
    "\n",
    "# Fractional error of the inferred solution compared to the input 'true' parameters\n",
    "m_frac_err = # complete\n",
    "b_frac_err = # complete\n",
    "print(f'Δm/m = {m_frac_err:.3f}, Δb/b = {b_frac_err:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginalize Over Each Parameter\n",
    "Since you have a grid of points, it is easy to integrate (sum) along the rows and columns of the grid. Use this method to find the marginalized posterior for each parameter and plot the results. How do the peaks of these distributions compare to our input parameters? (Hint: use the `axis` keyword in `np.sum`.) (Another hint: integrate the probability, not the log of the probability.)\n",
    "\n",
    "**Marginalization**\n",
    "\n",
    "If we just want to know the posterior for one parameter, we need to marginalize over all the possible values of the other parameter(s).\n",
    "\n",
    "$$ p(\\theta_1 | \\mathrm{data}) = \\int p(\\theta_1, \\theta_2 | \\mathrm{data}) \\, d\\theta_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_marginalized_m = # complete\n",
    "plt.figure()\n",
    "plt.plot(# complete\n",
    "\n",
    "posterior_marginalized_b = # complete\n",
    "plt.figure()\n",
    "plt.plot(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate it on a Random Sample (Monte Carlo)\n",
    "Instead of evaluating the posterior at every point on the grid, it can be more efficient to evaluate it on a random sample of points within the parameter space. Generate some random points using the `np.random` module and evaluate the posterior at each one. Plot them and color code by the posterior. (Use the `c` parameter of `plt.scatter`.)  Plot the input values of $m$ and $b$ as a point to see where they sit in the posterior space. How does this plot compare to the plot in the previous section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_random = # complete\n",
    "b_random = # complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "posterior_random = # complete\n",
    "\n",
    "plt.scatter(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, find the parameters that maximize the posterior distribution. How close did we come to the parameters we used to generate the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the sample where the posterior is maximised --> what m and b is this?\n",
    "i_max = np.argmax(posterior_random)\n",
    "m_infer = # complete\n",
    "b_infer = # complete\n",
    "print(f'm = {m_infer:.1f}, b = {b_infer:.1f}')\n",
    "\n",
    "# Fractional error of the inferred solution compared to the input 'true' parameters\n",
    "m_frac_err = # complete\n",
    "b_frac_err = # complete\n",
    "print(f'Δm/m = {m_frac_err:.3f}, Δb/b = {b_frac_err:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's complicated to calculate the marginalized posteriors using this method. You have to do some kind of binning or interpolation. Don't worry about that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-Away Message\n",
    "Both methods work, but the point is it's wasteful (and impossible for higher dimensons or large $N$) to be sampling the posterior when the probability density is $e^{-100000}$. Later, we will see a much more efficient sampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Things to Try\n",
    "- Redo the experiment but with a different prior. How much do your results change? Try plotting the priors on top of your marginalized posteriors.\n",
    "- Redo the experiment but with a much larger scatter. How well do you do? Now do the priors affect your results?\n",
    "- Add another parameter to your model: the intrinsic scatter. See if you can recover the value you used to generate the noisy data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
